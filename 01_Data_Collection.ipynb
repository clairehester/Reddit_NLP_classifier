{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 3 - Reddit NLP\n",
    "## 01-Data Collection\n",
    "\n",
    "This notebook was converted into a py script, \"reddit_pull_claire.py\", using nbcovert. This script was then run through the terminal. I used this Python script to scrape two subreddits for comparison: r/legaladvice and r/NoStupidQuestions. I pulled 30,000 submissions from each subreddit page. This was executed using a function designed to pull 100 comments at a time, with a 30 second sleep time in between each pull. This function was iterated over 300 times for each subreddit, creating two dataframes. These were then concatenated and exported to csv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standards\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# API\n",
    "import requests\n",
    "\n",
    "# Automating\n",
    "import time\n",
    "import datetime\n",
    "import warnings\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_posts(subreddit, n_iter, epoch_right_now): # subreddit name and number of times function should run\n",
    "    pass\n",
    "    # store base url variable\n",
    "    base_url = 'https://api.pushshift.io/reddit/search/submission/?subreddit='\n",
    "    \n",
    "    # instantiate empty list    \n",
    "    df_list = []\n",
    "    \n",
    "    # save current epoch, used to iterate in reverse through time\n",
    "    current_time = epoch_right_now\n",
    "    \n",
    "    # set up for loop\n",
    "    for post in range(n_iter):\n",
    "        \n",
    "        # instantiate get request\n",
    "        res = requests.get(\n",
    "            \n",
    "            # requests.get takes base_url and params\n",
    "            base_url,\n",
    "            \n",
    "            # parameters for get request\n",
    "            params = {  \n",
    "                \n",
    "                # specify subreddit\n",
    "                'subreddit': subreddit,\n",
    "                \n",
    "                # specify number of posts to pull\n",
    "                'size': 100,\n",
    "                \n",
    "                # ???\n",
    "                'lang': True,\n",
    "                \n",
    "                # pull everything from current time backward\n",
    "                'before': current_time }\n",
    "        )\n",
    "        \n",
    "        # take data from most recent request, store as df\n",
    "        df = pd.DataFrame(res.json()['data'])\n",
    "        \n",
    "        # pull specific columns from dataframe for analysis\n",
    "        df = df.loc[:, ['title',\n",
    "                'created_utc',\n",
    "                'selftext',\n",
    "                'subreddit',\n",
    "                'author',\n",
    "                'media_only',\n",
    "                'permalink']]\n",
    "        \n",
    "        # append to empty dataframe list\n",
    "        df_list.append(df)\n",
    "        \n",
    "        # add wait time - seconds in parentheses\n",
    "        time.sleep(10)\n",
    "        \n",
    "        # set current time counter back to last epoch in recently grabbed df\n",
    "        current_time = df['created_utc'].min()\n",
    "\n",
    "    # return one dataframe for all requests\n",
    "    return pd.concat(df_list, axis=0)\n",
    "\n",
    "# Adapated from Tim Book's Lesson Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "legal = get_posts('legaladvice',300, 1601524444)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "nsq = get_posts('NoStupidQuestions',300,1601524444)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "legal_nsq = pd.concat([legal, nsq])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "legal_nsq.to_csv('legal_nsq.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
